services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    environment:
      - USE_OLLAMA_DOCKER=False
      - OLLAMA_BASE_URL=http://your-ollama-host:11434
      - ENABLE_OPENAI_API=True
      - OPENAI_API_BASE_URL=https://openrouter.ai/api/v1
      - OPENAI_API_KEY=${OPENROUTER_API_KEY}
      - ENABLE_AUTOCOMPLETE_GENERATION=False
      - ENABLE_EVALUATION_ARENA_MODELS=False
      - RAG_OPENAI_API_BASE_URL=https://api.openai.com/v1
      - RAG_EMBEDDING_ENGINE=openai
      - RAG_OPENAI_API_KEY=${OPENAI_API_KEY}
      - RAG_EMBEDDING_MODEL=text-embedding-3-small
      - CHROMA_HTTP_HOST=chroma
      - ENABLE_RAG_WEB_SEARCH=True
      - RAG_WEB_SEARCH_ENGINE=searxng
      - RAG_WEB_SEARCH_RESULT_COUNT=3
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=10
      - SEARXNG_QUERY_URL=http://searxng:8080/search?q=<query>
    volumes:
      - ${DATA_DIR}/open-webui:/app/backend/data
    networks:
      - openwebui
  
  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - "4000:4000"
    environment:
        LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
        LITELLM_SALT_KEY: ${LITELLM_SALT_KEY}
        DATABASE_URL: "postgresql://llmproxy:dbpassword9090@litellm_db:5432/litellm"
        STORE_MODEL_IN_DB: "True"
    networks:
      - openwebui

  db:
    container_name: litellm_db
    image: postgres:16
    restart: unless-stopped
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    volumes:
      - postgres_data:/var/lib/postgresql/data  # Persists Postgres data across container restarts
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 1s
      timeout: 5s
      retries: 10
    networks:
      - openwebui

  chroma:
    container_name: chroma
    image: chromadb/chroma:latest
    volumes:
      - ${DATA_DIR}/chroma:/chroma/chroma
    restart: unless-stopped
    networks:
      - openwebui

  cloudflare:
    container_name: cloudflare_owui
    image: cloudflare/cloudflared:latest
    restart: unless-stopped
    command: tunnel run
    networks:
      - openwebui
    environment:
      - TUNNEL_TOKEN=${TUNNEL_TOKEN}

  searxng:
    container_name: searxng
    image: searxng/searxng:latest
    ports:
      - "8080:8080"
    volumes:
      - ${DATA_DIR}/searxng:/etc/searxng:rw
    environment:
      - SEARXNG_HOSTNAME=${SEARXNG_HOSTNAME}
    restart: unless-stopped
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    networks:
      - openwebui
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  caddy:
    container_name: caddy
    image: docker.io/library/caddy:2-alpine
    network_mode: host
    restart: unless-stopped
    volumes:
      - ${DATA_DIR}/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy-data:/data:rw
      - caddy-config:/config:rw
    environment:
      - SEARXNG_HOSTNAME=${SEARXNG_HOSTNAME:-http://localhost}
      - SEARXNG_TLS=${LETSENCRYPT_EMAIL:-internal}
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  redis:
    container_name: redis
    image: docker.io/valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    networks:
      - openwebui
    volumes:
      - valkey-data2:/data
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

volumes:
  caddy-data:
  caddy-config:
  valkey-data2:
  postgres_data:
    name: litellm_postgres_data

networks:
  openwebui:
    driver: bridge